{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 운동 동작 분류 AI 경진대회\n",
    "### [ 국경원 요원 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개발 환경\n",
    "- OS : Windows 10 Pro\n",
    "- Python Ver : 3.8.5\n",
    "- tensorflow Ver: 2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차\n",
    "<font color=\"red\"><Br>\n",
    "1.모듈 및 파일 로드<br>\n",
    "2.설명변수 생성 및 전처리<br>\n",
    "3.학습및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><Br>\n",
    "# 1.모듈 및 파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.0\n2.4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매번 모델링을 할 때마다 동일한 결과를 얻으려면 아래 코드를 실행\n",
    "def reset_seeds(seed, reset_graph_with_backend=None):\n",
    "    if reset_graph_with_backend is not None:\n",
    "        K = reset_graph_with_backend\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        print(\"KERAS AND TENSORFLOW GRAPHS RESET\")  \n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed+100)\n",
    "    tf.compat.v1.set_random_seed(seed+200)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''  \n",
    "    print(\"RANDOM SEEDS RESET {}\".format(seed))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM SEEDS RESET 1\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "reset_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./\"\n",
    "SUB_PATH = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(f'{DATA_PATH}train_features.csv')\n",
    "train_labels=pd.read_csv(f'{DATA_PATH}train_labels.csv')\n",
    "test=pd.read_csv(f'{DATA_PATH}test_features.csv')\n",
    "submission=pd.read_csv(f'{DATA_PATH}sample_submission.csv')\n",
    "\n",
    "# 진행률 확인\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><Br>\n",
    "# 2.설명변수 생성 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 별도의 히든레이어에 넣을 설명변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_id = pd.DataFrame(train.id.unique(),columns=[\"id\"])\n",
    "te_id = pd.DataFrame(test.id.unique(),columns=[\"id\"])\n",
    "ft_train = tr_id.copy()\n",
    "ft_test = te_id.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 변수에 대한 max,min,mean,std 집계를 하여 설명변수를 생성\n",
    "- 각 변수에 기울기에 대한 mean , std ,sum 집계를 하여 설명변수를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id']\n",
    "cols.extend(train.iloc[:,2:].columns.tolist())\n",
    "f_list = [\n",
    "            ('max', 'max' ), \n",
    "            ('min', 'min' ),\n",
    "            ('mean', 'mean' ),\n",
    "            ('std', 'std' ),\n",
    "            (\n",
    "            'gradient_mean', \n",
    "             lambda x : np.gradient(x).mean()\n",
    "            ), \n",
    "            (\n",
    "            'gradient_std', \n",
    "             lambda x : np.gradient(x).std()\n",
    "            ), \n",
    "            \n",
    "             (\n",
    "            'gradient_sum', \n",
    "             lambda x : np.gradient(x).sum()\n",
    "            ),\n",
    "            \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id  acc_x_max  acc_x_min  acc_x_mean  acc_x_std  acc_x_gradient_mean  \\\n",
       "0        0   1.344268   0.591940    0.931329   0.191479             0.000044   \n",
       "1        1   1.234020  -2.156208   -0.766580   0.495528             0.000522   \n",
       "2        2   1.219836  -1.142847    0.039836   0.711972            -0.001042   \n",
       "3        3  -0.622250  -1.417751   -0.887702   0.130899             0.000379   \n",
       "4        4   0.599720  -2.429109   -0.659018   0.495170            -0.001961   \n",
       "...    ...        ...        ...         ...        ...                  ...   \n",
       "3120  3120   0.390798  -1.624711   -0.300454   0.403175            -0.000358   \n",
       "3121  3121  -0.446650  -1.575455   -0.974298   0.169963            -0.000620   \n",
       "3122  3122   0.744666  -2.578974   -1.114246   0.683789            -0.000555   \n",
       "3123  3123   0.915846  -0.929133   -0.111333   0.432722            -0.000964   \n",
       "3124  3124   0.538809  -1.013813   -0.434048   0.522107             0.000534   \n",
       "\n",
       "      acc_x_gradient_std  acc_x_gradient_sum  acc_y_max  acc_y_min  ...  \\\n",
       "0               0.036163            0.026181   0.176871  -0.624113  ...   \n",
       "1               0.107822            0.313130   0.700065  -1.295598  ...   \n",
       "2               0.092330           -0.625047   0.650645  -0.690990  ...   \n",
       "3               0.046002            0.227675   0.283721  -0.540827  ...   \n",
       "4               0.090496           -1.176893   1.724782  -2.055076  ...   \n",
       "...                  ...                 ...        ...        ...  ...   \n",
       "3120            0.084204           -0.215055   0.168070  -1.289257  ...   \n",
       "3121            0.038752           -0.371789   0.117965  -0.609743  ...   \n",
       "3122            0.129473           -0.332854   1.268138  -2.036646  ...   \n",
       "3123            0.057924           -0.578330   1.473727   0.272406  ...   \n",
       "3124            0.029587            0.320543  -0.193247  -0.952761  ...   \n",
       "\n",
       "      gy_y_gradient_mean  gy_y_gradient_std  gy_y_gradient_sum    gy_z_max  \\\n",
       "0               0.074032           4.366987          44.419041   55.953827   \n",
       "1              -0.135346          16.600487         -81.207515  340.170199   \n",
       "2               0.541111          13.716743         324.666586   55.642836   \n",
       "3              -0.040955           8.919657         -24.573111   56.456908   \n",
       "4              -0.562629          15.199774        -337.577444  221.015193   \n",
       "...                  ...                ...                ...         ...   \n",
       "3120            0.037671           9.503145          22.602452  121.958427   \n",
       "3121            0.023935           2.937453          14.361012   57.349878   \n",
       "3122            0.940091          22.509199         564.054404  453.943910   \n",
       "3123           -0.315875          12.454230        -189.525145  310.558507   \n",
       "3124           -0.025215           3.993583         -15.128745   80.302840   \n",
       "\n",
       "        gy_z_min  gy_z_mean    gy_z_std  gy_z_gradient_mean  \\\n",
       "0     -79.930029   1.182107   25.275185            0.121532   \n",
       "1    -270.980823   1.393294   75.545343           -0.047440   \n",
       "2     -44.192071   3.053291   13.920337            0.049267   \n",
       "3     -85.600536  -5.869898   23.647153            0.113235   \n",
       "4    -270.581913   4.453382   46.148326           -0.570786   \n",
       "...          ...        ...         ...                 ...   \n",
       "3120  -79.392292  -0.054026   24.913819            0.026928   \n",
       "3121  -39.777626  -2.792238   12.786464            0.010601   \n",
       "3122 -247.908573  -1.722830  131.916609           -0.624373   \n",
       "3123 -206.580638  -5.930252   71.243150            0.049862   \n",
       "3124 -100.845205  -3.246825   49.371117           -0.029993   \n",
       "\n",
       "      gy_z_gradient_std  gy_z_gradient_sum  \n",
       "0              3.727328          72.919282  \n",
       "1             14.139488         -28.464187  \n",
       "2              5.572570          29.560015  \n",
       "3              4.752998          67.940966  \n",
       "4             14.034324        -342.471783  \n",
       "...                 ...                ...  \n",
       "3120           6.510302          16.156778  \n",
       "3121           2.786408           6.360638  \n",
       "3122          14.682723        -374.623971  \n",
       "3123          10.912310          29.917346  \n",
       "3124           3.032202         -17.995845  \n",
       "\n",
       "[3125 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>acc_x_max</th>\n      <th>acc_x_min</th>\n      <th>acc_x_mean</th>\n      <th>acc_x_std</th>\n      <th>acc_x_gradient_mean</th>\n      <th>acc_x_gradient_std</th>\n      <th>acc_x_gradient_sum</th>\n      <th>acc_y_max</th>\n      <th>acc_y_min</th>\n      <th>...</th>\n      <th>gy_y_gradient_mean</th>\n      <th>gy_y_gradient_std</th>\n      <th>gy_y_gradient_sum</th>\n      <th>gy_z_max</th>\n      <th>gy_z_min</th>\n      <th>gy_z_mean</th>\n      <th>gy_z_std</th>\n      <th>gy_z_gradient_mean</th>\n      <th>gy_z_gradient_std</th>\n      <th>gy_z_gradient_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.344268</td>\n      <td>0.591940</td>\n      <td>0.931329</td>\n      <td>0.191479</td>\n      <td>0.000044</td>\n      <td>0.036163</td>\n      <td>0.026181</td>\n      <td>0.176871</td>\n      <td>-0.624113</td>\n      <td>...</td>\n      <td>0.074032</td>\n      <td>4.366987</td>\n      <td>44.419041</td>\n      <td>55.953827</td>\n      <td>-79.930029</td>\n      <td>1.182107</td>\n      <td>25.275185</td>\n      <td>0.121532</td>\n      <td>3.727328</td>\n      <td>72.919282</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.234020</td>\n      <td>-2.156208</td>\n      <td>-0.766580</td>\n      <td>0.495528</td>\n      <td>0.000522</td>\n      <td>0.107822</td>\n      <td>0.313130</td>\n      <td>0.700065</td>\n      <td>-1.295598</td>\n      <td>...</td>\n      <td>-0.135346</td>\n      <td>16.600487</td>\n      <td>-81.207515</td>\n      <td>340.170199</td>\n      <td>-270.980823</td>\n      <td>1.393294</td>\n      <td>75.545343</td>\n      <td>-0.047440</td>\n      <td>14.139488</td>\n      <td>-28.464187</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.219836</td>\n      <td>-1.142847</td>\n      <td>0.039836</td>\n      <td>0.711972</td>\n      <td>-0.001042</td>\n      <td>0.092330</td>\n      <td>-0.625047</td>\n      <td>0.650645</td>\n      <td>-0.690990</td>\n      <td>...</td>\n      <td>0.541111</td>\n      <td>13.716743</td>\n      <td>324.666586</td>\n      <td>55.642836</td>\n      <td>-44.192071</td>\n      <td>3.053291</td>\n      <td>13.920337</td>\n      <td>0.049267</td>\n      <td>5.572570</td>\n      <td>29.560015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-0.622250</td>\n      <td>-1.417751</td>\n      <td>-0.887702</td>\n      <td>0.130899</td>\n      <td>0.000379</td>\n      <td>0.046002</td>\n      <td>0.227675</td>\n      <td>0.283721</td>\n      <td>-0.540827</td>\n      <td>...</td>\n      <td>-0.040955</td>\n      <td>8.919657</td>\n      <td>-24.573111</td>\n      <td>56.456908</td>\n      <td>-85.600536</td>\n      <td>-5.869898</td>\n      <td>23.647153</td>\n      <td>0.113235</td>\n      <td>4.752998</td>\n      <td>67.940966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.599720</td>\n      <td>-2.429109</td>\n      <td>-0.659018</td>\n      <td>0.495170</td>\n      <td>-0.001961</td>\n      <td>0.090496</td>\n      <td>-1.176893</td>\n      <td>1.724782</td>\n      <td>-2.055076</td>\n      <td>...</td>\n      <td>-0.562629</td>\n      <td>15.199774</td>\n      <td>-337.577444</td>\n      <td>221.015193</td>\n      <td>-270.581913</td>\n      <td>4.453382</td>\n      <td>46.148326</td>\n      <td>-0.570786</td>\n      <td>14.034324</td>\n      <td>-342.471783</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3120</th>\n      <td>3120</td>\n      <td>0.390798</td>\n      <td>-1.624711</td>\n      <td>-0.300454</td>\n      <td>0.403175</td>\n      <td>-0.000358</td>\n      <td>0.084204</td>\n      <td>-0.215055</td>\n      <td>0.168070</td>\n      <td>-1.289257</td>\n      <td>...</td>\n      <td>0.037671</td>\n      <td>9.503145</td>\n      <td>22.602452</td>\n      <td>121.958427</td>\n      <td>-79.392292</td>\n      <td>-0.054026</td>\n      <td>24.913819</td>\n      <td>0.026928</td>\n      <td>6.510302</td>\n      <td>16.156778</td>\n    </tr>\n    <tr>\n      <th>3121</th>\n      <td>3121</td>\n      <td>-0.446650</td>\n      <td>-1.575455</td>\n      <td>-0.974298</td>\n      <td>0.169963</td>\n      <td>-0.000620</td>\n      <td>0.038752</td>\n      <td>-0.371789</td>\n      <td>0.117965</td>\n      <td>-0.609743</td>\n      <td>...</td>\n      <td>0.023935</td>\n      <td>2.937453</td>\n      <td>14.361012</td>\n      <td>57.349878</td>\n      <td>-39.777626</td>\n      <td>-2.792238</td>\n      <td>12.786464</td>\n      <td>0.010601</td>\n      <td>2.786408</td>\n      <td>6.360638</td>\n    </tr>\n    <tr>\n      <th>3122</th>\n      <td>3122</td>\n      <td>0.744666</td>\n      <td>-2.578974</td>\n      <td>-1.114246</td>\n      <td>0.683789</td>\n      <td>-0.000555</td>\n      <td>0.129473</td>\n      <td>-0.332854</td>\n      <td>1.268138</td>\n      <td>-2.036646</td>\n      <td>...</td>\n      <td>0.940091</td>\n      <td>22.509199</td>\n      <td>564.054404</td>\n      <td>453.943910</td>\n      <td>-247.908573</td>\n      <td>-1.722830</td>\n      <td>131.916609</td>\n      <td>-0.624373</td>\n      <td>14.682723</td>\n      <td>-374.623971</td>\n    </tr>\n    <tr>\n      <th>3123</th>\n      <td>3123</td>\n      <td>0.915846</td>\n      <td>-0.929133</td>\n      <td>-0.111333</td>\n      <td>0.432722</td>\n      <td>-0.000964</td>\n      <td>0.057924</td>\n      <td>-0.578330</td>\n      <td>1.473727</td>\n      <td>0.272406</td>\n      <td>...</td>\n      <td>-0.315875</td>\n      <td>12.454230</td>\n      <td>-189.525145</td>\n      <td>310.558507</td>\n      <td>-206.580638</td>\n      <td>-5.930252</td>\n      <td>71.243150</td>\n      <td>0.049862</td>\n      <td>10.912310</td>\n      <td>29.917346</td>\n    </tr>\n    <tr>\n      <th>3124</th>\n      <td>3124</td>\n      <td>0.538809</td>\n      <td>-1.013813</td>\n      <td>-0.434048</td>\n      <td>0.522107</td>\n      <td>0.000534</td>\n      <td>0.029587</td>\n      <td>0.320543</td>\n      <td>-0.193247</td>\n      <td>-0.952761</td>\n      <td>...</td>\n      <td>-0.025215</td>\n      <td>3.993583</td>\n      <td>-15.128745</td>\n      <td>80.302840</td>\n      <td>-100.845205</td>\n      <td>-3.246825</td>\n      <td>49.371117</td>\n      <td>-0.029993</td>\n      <td>3.032202</td>\n      <td>-17.995845</td>\n    </tr>\n  </tbody>\n</table>\n<p>3125 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 학습 데이터\n",
    "f_ = train[cols].groupby(\"id\").agg(f_list)\n",
    "f_.columns = [ f\"{c1}_{c2}\"  for c1, c2 in f_.columns ]\n",
    "f_ = f_.reset_index()\n",
    "f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_train = pd.merge(ft_train,f_,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       id  acc_x_max  acc_x_min  acc_x_mean  acc_x_std  acc_x_gradient_mean  \\\n",
       "0    3125  -0.275446  -1.564000   -1.018731   0.236232            -0.000607   \n",
       "1    3126   0.627571  -1.929033   -0.522843   0.539688            -0.002408   \n",
       "2    3127   2.972063  -0.792916    0.506947   0.219934            -0.000619   \n",
       "3    3128   0.337281  -1.045889   -0.577603   0.431713             0.000151   \n",
       "4    3129   0.015642  -2.153047   -0.738640   0.305797            -0.000529   \n",
       "..    ...        ...        ...         ...        ...                  ...   \n",
       "777  3902   0.427159  -2.050254   -0.907299   0.352604            -0.000200   \n",
       "778  3903   1.659451  -1.709527   -0.608731   0.663522             0.002253   \n",
       "779  3904  -0.085249  -2.124959   -0.753193   0.252666             0.000170   \n",
       "780  3905   1.438345   0.536568    0.958903   0.164880            -0.000717   \n",
       "781  3906   0.224194  -1.534616   -0.781752   0.323766             0.001667   \n",
       "\n",
       "     acc_x_gradient_std  acc_x_gradient_sum  acc_y_max  acc_y_min  ...  \\\n",
       "0              0.052546           -0.364450   0.228040  -0.470937  ...   \n",
       "1              0.065952           -1.444505   1.708743  -0.200678  ...   \n",
       "2              0.136834           -0.371407   1.941820   0.219008  ...   \n",
       "3              0.024039            0.090362  -0.258476  -1.294482  ...   \n",
       "4              0.090366           -0.317549   1.562602  -0.860883  ...   \n",
       "..                  ...                 ...        ...        ...  ...   \n",
       "777            0.105253           -0.120011   3.057501  -1.414874  ...   \n",
       "778            0.098773            1.351889   1.549890  -1.247963  ...   \n",
       "779            0.067927            0.101945   1.236138  -0.443533  ...   \n",
       "780            0.053917           -0.430454   0.076427  -0.580191  ...   \n",
       "781            0.054858            0.999939   0.144179  -0.958351  ...   \n",
       "\n",
       "     gy_y_gradient_mean  gy_y_gradient_std  gy_y_gradient_sum    gy_z_max  \\\n",
       "0             -0.128438           5.668033         -77.062604   49.981455   \n",
       "1              0.073970           7.995967          44.381857  169.417650   \n",
       "2              0.087458           6.366008          52.474878   97.211730   \n",
       "3             -0.044294           5.093294         -26.576601  167.860762   \n",
       "4             -0.080270          13.603918         -48.161825  138.130133   \n",
       "..                  ...                ...                ...         ...   \n",
       "777           -0.387381          15.964694        -232.428525  214.192019   \n",
       "778            0.189548          11.132063         113.728549  253.689077   \n",
       "779            0.202383           6.845427         121.429570  251.344358   \n",
       "780            0.071671           7.192561          43.002303   58.041427   \n",
       "781           -0.063213           7.004929         -37.927646  101.996940   \n",
       "\n",
       "       gy_z_min  gy_z_mean   gy_z_std  gy_z_gradient_mean  gy_z_gradient_std  \\\n",
       "0    -35.446915  -2.000683  12.251648           -0.006010           3.289276   \n",
       "1   -147.597574  -3.604579  61.604867            0.193885           7.745503   \n",
       "2   -154.477074  -0.393175  23.041463           -0.030464           9.410068   \n",
       "3   -117.297766  -0.024318  37.967372           -0.009246           4.504459   \n",
       "4   -125.598600   5.745498  43.353007           -0.257599           9.722807   \n",
       "..          ...        ...        ...                 ...                ...   \n",
       "777 -241.419881  -0.429678  68.462972            0.078550          12.926505   \n",
       "778 -164.337764  13.272141  81.398849           -0.068578          14.228869   \n",
       "779 -150.012379   1.935083  48.113344           -0.146594           8.700739   \n",
       "780  -51.905231   0.932313  26.523530            0.066879           5.021503   \n",
       "781 -190.358982  -7.197783  36.491662           -0.159554           6.539498   \n",
       "\n",
       "     gy_z_gradient_sum  \n",
       "0            -3.605857  \n",
       "1           116.331005  \n",
       "2           -18.278557  \n",
       "3            -5.547546  \n",
       "4          -154.559603  \n",
       "..                 ...  \n",
       "777          47.130277  \n",
       "778         -41.146929  \n",
       "779         -87.956111  \n",
       "780          40.127611  \n",
       "781         -95.732526  \n",
       "\n",
       "[782 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>acc_x_max</th>\n      <th>acc_x_min</th>\n      <th>acc_x_mean</th>\n      <th>acc_x_std</th>\n      <th>acc_x_gradient_mean</th>\n      <th>acc_x_gradient_std</th>\n      <th>acc_x_gradient_sum</th>\n      <th>acc_y_max</th>\n      <th>acc_y_min</th>\n      <th>...</th>\n      <th>gy_y_gradient_mean</th>\n      <th>gy_y_gradient_std</th>\n      <th>gy_y_gradient_sum</th>\n      <th>gy_z_max</th>\n      <th>gy_z_min</th>\n      <th>gy_z_mean</th>\n      <th>gy_z_std</th>\n      <th>gy_z_gradient_mean</th>\n      <th>gy_z_gradient_std</th>\n      <th>gy_z_gradient_sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3125</td>\n      <td>-0.275446</td>\n      <td>-1.564000</td>\n      <td>-1.018731</td>\n      <td>0.236232</td>\n      <td>-0.000607</td>\n      <td>0.052546</td>\n      <td>-0.364450</td>\n      <td>0.228040</td>\n      <td>-0.470937</td>\n      <td>...</td>\n      <td>-0.128438</td>\n      <td>5.668033</td>\n      <td>-77.062604</td>\n      <td>49.981455</td>\n      <td>-35.446915</td>\n      <td>-2.000683</td>\n      <td>12.251648</td>\n      <td>-0.006010</td>\n      <td>3.289276</td>\n      <td>-3.605857</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3126</td>\n      <td>0.627571</td>\n      <td>-1.929033</td>\n      <td>-0.522843</td>\n      <td>0.539688</td>\n      <td>-0.002408</td>\n      <td>0.065952</td>\n      <td>-1.444505</td>\n      <td>1.708743</td>\n      <td>-0.200678</td>\n      <td>...</td>\n      <td>0.073970</td>\n      <td>7.995967</td>\n      <td>44.381857</td>\n      <td>169.417650</td>\n      <td>-147.597574</td>\n      <td>-3.604579</td>\n      <td>61.604867</td>\n      <td>0.193885</td>\n      <td>7.745503</td>\n      <td>116.331005</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3127</td>\n      <td>2.972063</td>\n      <td>-0.792916</td>\n      <td>0.506947</td>\n      <td>0.219934</td>\n      <td>-0.000619</td>\n      <td>0.136834</td>\n      <td>-0.371407</td>\n      <td>1.941820</td>\n      <td>0.219008</td>\n      <td>...</td>\n      <td>0.087458</td>\n      <td>6.366008</td>\n      <td>52.474878</td>\n      <td>97.211730</td>\n      <td>-154.477074</td>\n      <td>-0.393175</td>\n      <td>23.041463</td>\n      <td>-0.030464</td>\n      <td>9.410068</td>\n      <td>-18.278557</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3128</td>\n      <td>0.337281</td>\n      <td>-1.045889</td>\n      <td>-0.577603</td>\n      <td>0.431713</td>\n      <td>0.000151</td>\n      <td>0.024039</td>\n      <td>0.090362</td>\n      <td>-0.258476</td>\n      <td>-1.294482</td>\n      <td>...</td>\n      <td>-0.044294</td>\n      <td>5.093294</td>\n      <td>-26.576601</td>\n      <td>167.860762</td>\n      <td>-117.297766</td>\n      <td>-0.024318</td>\n      <td>37.967372</td>\n      <td>-0.009246</td>\n      <td>4.504459</td>\n      <td>-5.547546</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3129</td>\n      <td>0.015642</td>\n      <td>-2.153047</td>\n      <td>-0.738640</td>\n      <td>0.305797</td>\n      <td>-0.000529</td>\n      <td>0.090366</td>\n      <td>-0.317549</td>\n      <td>1.562602</td>\n      <td>-0.860883</td>\n      <td>...</td>\n      <td>-0.080270</td>\n      <td>13.603918</td>\n      <td>-48.161825</td>\n      <td>138.130133</td>\n      <td>-125.598600</td>\n      <td>5.745498</td>\n      <td>43.353007</td>\n      <td>-0.257599</td>\n      <td>9.722807</td>\n      <td>-154.559603</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>777</th>\n      <td>3902</td>\n      <td>0.427159</td>\n      <td>-2.050254</td>\n      <td>-0.907299</td>\n      <td>0.352604</td>\n      <td>-0.000200</td>\n      <td>0.105253</td>\n      <td>-0.120011</td>\n      <td>3.057501</td>\n      <td>-1.414874</td>\n      <td>...</td>\n      <td>-0.387381</td>\n      <td>15.964694</td>\n      <td>-232.428525</td>\n      <td>214.192019</td>\n      <td>-241.419881</td>\n      <td>-0.429678</td>\n      <td>68.462972</td>\n      <td>0.078550</td>\n      <td>12.926505</td>\n      <td>47.130277</td>\n    </tr>\n    <tr>\n      <th>778</th>\n      <td>3903</td>\n      <td>1.659451</td>\n      <td>-1.709527</td>\n      <td>-0.608731</td>\n      <td>0.663522</td>\n      <td>0.002253</td>\n      <td>0.098773</td>\n      <td>1.351889</td>\n      <td>1.549890</td>\n      <td>-1.247963</td>\n      <td>...</td>\n      <td>0.189548</td>\n      <td>11.132063</td>\n      <td>113.728549</td>\n      <td>253.689077</td>\n      <td>-164.337764</td>\n      <td>13.272141</td>\n      <td>81.398849</td>\n      <td>-0.068578</td>\n      <td>14.228869</td>\n      <td>-41.146929</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>3904</td>\n      <td>-0.085249</td>\n      <td>-2.124959</td>\n      <td>-0.753193</td>\n      <td>0.252666</td>\n      <td>0.000170</td>\n      <td>0.067927</td>\n      <td>0.101945</td>\n      <td>1.236138</td>\n      <td>-0.443533</td>\n      <td>...</td>\n      <td>0.202383</td>\n      <td>6.845427</td>\n      <td>121.429570</td>\n      <td>251.344358</td>\n      <td>-150.012379</td>\n      <td>1.935083</td>\n      <td>48.113344</td>\n      <td>-0.146594</td>\n      <td>8.700739</td>\n      <td>-87.956111</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>3905</td>\n      <td>1.438345</td>\n      <td>0.536568</td>\n      <td>0.958903</td>\n      <td>0.164880</td>\n      <td>-0.000717</td>\n      <td>0.053917</td>\n      <td>-0.430454</td>\n      <td>0.076427</td>\n      <td>-0.580191</td>\n      <td>...</td>\n      <td>0.071671</td>\n      <td>7.192561</td>\n      <td>43.002303</td>\n      <td>58.041427</td>\n      <td>-51.905231</td>\n      <td>0.932313</td>\n      <td>26.523530</td>\n      <td>0.066879</td>\n      <td>5.021503</td>\n      <td>40.127611</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>3906</td>\n      <td>0.224194</td>\n      <td>-1.534616</td>\n      <td>-0.781752</td>\n      <td>0.323766</td>\n      <td>0.001667</td>\n      <td>0.054858</td>\n      <td>0.999939</td>\n      <td>0.144179</td>\n      <td>-0.958351</td>\n      <td>...</td>\n      <td>-0.063213</td>\n      <td>7.004929</td>\n      <td>-37.927646</td>\n      <td>101.996940</td>\n      <td>-190.358982</td>\n      <td>-7.197783</td>\n      <td>36.491662</td>\n      <td>-0.159554</td>\n      <td>6.539498</td>\n      <td>-95.732526</td>\n    </tr>\n  </tbody>\n</table>\n<p>782 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 테스트 데이터\n",
    "f_ = test[cols].groupby(\"id\").agg(f_list)\n",
    "f_.columns = [ f\"{c1}_{c2}\"  for c1, c2 in f_.columns ]\n",
    "f_ = f_.reset_index()\n",
    "f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_test = pd.merge(ft_test,f_,how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 트랜스포머의 인코더에 추가로 넣을 설명변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datamanim 님이 코드 공유 게시판에 공유해주신 설명변수 추가 (감사합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터\n",
    "train['acc_t']  =(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/3)\n",
    "# 테스트 데이터\n",
    "test['acc_t']  =(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 변수의 차분값을 구하여 설명변수를 생성 (첫번째 값은 0으로 대체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3125/3125 [00:01<00:00, 2181.75it/s]\n",
      "100%|██████████| 782/782 [00:00<00:00, 2226.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터\n",
    "f_ = train.groupby(\"id\").progress_apply(\n",
    "    lambda x : x.iloc[:,2:].diff().fillna(0)\n",
    ").add_prefix(\"diff_\")\n",
    "\n",
    "train = pd.concat([train,f_],axis=1)\n",
    "\n",
    "# 테스트 데이터\n",
    "f_ = test.groupby(\"id\").progress_apply(\n",
    "    lambda x : x.iloc[:,2:].diff().fillna(0)\n",
    ").add_prefix(\"diff_\")\n",
    "\n",
    "test = pd.concat([test,f_],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 스케일 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sc = StandardScaler()\n",
    "train.iloc[:,2:] = ft_sc.fit_transform(train.iloc[:,2:]) # 학습 데이터\n",
    "test.iloc[:,2:] = ft_sc.transform(test.iloc[:,2:]) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_sc = StandardScaler()\n",
    "ft_train = ft_sc.fit_transform(ft_train.iloc[:,1:]) # 학습 데이터\n",
    "ft_test = ft_sc.transform(ft_test.iloc[:,1:]) # 테스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 학습및 테스트 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3125, 600, 14), (782, 600, 14), (3125, 61))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "ft_cnt = train.iloc[:,2:].columns.shape[0] # 설명변수 개수\n",
    "X_train = np.array(train.iloc[:,2:])\n",
    "X_test = np.array(test.iloc[:,2:])\n",
    "\n",
    "#차원 변경\n",
    "X_train = X_train.reshape(-1, 600, ft_cnt)\n",
    "X_test=X_test.reshape(-1, 600, ft_cnt)\n",
    "\n",
    "y = tf.keras.utils.to_categorical(train_labels['label']) \n",
    "\n",
    "X_train.shape , X_test.shape , y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><Br>\n",
    "# 3.학습및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음의 링크를 참고 하여 Transformer를 활용 하였습니다.\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs,node,drop_rate,activation):\n",
    "    attn_output = keras.layers.MultiHeadAttention(num_heads=2, \n",
    "                                        key_dim=node)(inputs, inputs)\n",
    "    attn_output = keras.layers.Dropout(drop_rate)(attn_output)\n",
    "    out1 = keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    ffn_output = keras.layers.Dense(node, activation=activation)(out1) #\n",
    "    ffn_output = keras.layers.Dense(node)(ffn_output) #\n",
    "    ffn_output = keras.layers.Dropout(drop_rate)(ffn_output)\n",
    "    return keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 3개의 인풋을 받아 학습을 진행\n",
    "- 2개의 인풋은 CNN 을 거쳐 트랜스포머의 인코더의 인풋으로 들어감\n",
    "- 1개의 인풋은 별도의 간단한 히든레이어에 들어감\n",
    "- 3개의 아웃풋을 에버리지 레이어를 통과후 소프트맥스를 통해 최종 예측값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dnn_model(node=64,activation='relu', drop_rate = 0.2 ,loss=\"categorical_crossentropy\", \n",
    "                 optimizer=\"rmsprop\",metrics=['accuracy']):\n",
    "    \n",
    "    avg_list = []\n",
    "    inputs_list = []\n",
    "    for i in range(3):\n",
    "        if i < 2:\n",
    "            inputs = keras.Input(shape=(600, 7))\n",
    "\n",
    "            x = keras.layers.Conv1D(node*2, 5, activation=activation)(inputs)\n",
    "            x = keras.layers.MaxPooling1D(3)(x)\n",
    "            x = keras.layers.Dropout(drop_rate)(x)\n",
    "            x = keras.layers.Conv1D(node, 5, activation=activation)(x)\n",
    "            x = keras.layers.MaxPooling1D(3)(x)\n",
    "            x = keras.layers.Dropout(drop_rate)(x) \n",
    "\n",
    "            positions = tf.range(start=0, limit=x.shape[1], delta=1,dtype=\"float32\") \n",
    "            positions = keras.layers.Embedding(input_dim=x.shape[1], output_dim=node)(positions)\n",
    "            x = x + positions\n",
    "\n",
    "            x = transformer_block(x,node,drop_rate,activation)\n",
    "            x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "            x = keras.layers.Dropout(drop_rate)(x)\n",
    "            avg_list.append(x)\n",
    "        else:\n",
    "            inputs = keras.Input(shape=(42,))\n",
    "            x = inputs\n",
    "            x = keras.layers.Dense(node, activation=activation)(x)\n",
    "            x = keras.layers.Dropout(drop_rate)(x)\n",
    "            x = keras.layers.Dense(node, activation='softmax')(x)\n",
    "            avg_list.append(x)\n",
    "            \n",
    "        inputs_list.append(inputs)\n",
    "    \n",
    "    x = keras.layers.Average()(avg_list)\n",
    "    \n",
    "    outputs = keras.layers.Dense(61, activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs_list, outputs=outputs)\n",
    "    model.compile(loss=loss, optimizer=optimizer,metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-4020e4f2de18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_dnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-42261fe6fc2d>\u001b[0m in \u001b[0;36mmy_dnn_model\u001b[1;34m(node, activation, drop_rate, loss, optimizer, metrics)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpositions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-49d355408b76>\u001b[0m in \u001b[0;36mtransformer_block\u001b[1;34m(inputs, node, drop_rate, activation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransformer_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     attn_output = keras.layers.MultiHeadAttention(num_heads=2, \n\u001b[0m\u001b[0;32m      3\u001b[0m                                         key_dim=node)(inputs, inputs)\n\u001b[0;32m      4\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'"
     ]
    }
   ],
   "source": [
    "model = my_dnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DACON.Dobby님이 공유해주신 증강코드를 수정하여 함수로 생성(감사합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_data(data , data_name, n=0 ,shift=False,list_ = False):\n",
    "    \"\"\"데이터 증강 함수\n",
    "            Args:\n",
    "                data (numpy array): 증강할 데이터\n",
    "                data_name (str): print 용\n",
    "                n (int):  증강 데이터 세트수\n",
    "                shift (bool): shift 사용 여부\n",
    "                list_ (bool):  list 묶음으로 반환 여부\n",
    "            Returns: \n",
    "                numpy array or list:\n",
    "    \"\"\"\n",
    "    data_ = data.copy()\n",
    "    if list_:\n",
    "        data_ = [data]\n",
    "    print(f\"##### {data_name} 데이터 {n} 개 증강... #####\")\n",
    "    for _ in range(n):\n",
    "        if shift:\n",
    "            shift_n = int(random.random()*600)\n",
    "            print(f\"shift num : {shift_n}\")\n",
    "            r_idx = np.roll(np.arange(600), shift_n)\n",
    "            \n",
    "            if list_:\n",
    "                data_.append(np.array(data[:,r_idx], np.float32))\n",
    "            else:\n",
    "                data_ = np.concatenate(  ( data_, np.array(data[:,r_idx], np.float32) ),axis=0 )\n",
    "        else:\n",
    "            if list_:\n",
    "                data_.append(data)\n",
    "            else:\n",
    "                data_ = np.concatenate(  ( data_, data ),axis=0 )\n",
    "    print(\"# 완료!!\")    \n",
    "    return data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 학습및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습데이터를 증강하여 학습진행(검증데이터 증강 X)\n",
    "- 검증은 CV 각 폴드 과정에서 검증데이터 증강후에 데이터 각 세트에 대해 예측을 진행후 나온 예측값을 산술평균후에 logloss 값 확인 및 저장\n",
    "- 테스트 데이터에 대한 각 폴드에 예측값의 경우도 검증과 동일하게 예측값 생성\n",
    "- CV 5fold 로 생성된 예측값을 산술평균 후에 최종 예측값으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM SEEDS RESET 1\n",
      "##### X_test 데이터 10 개 증강... #####\n",
      "shift num : 348\n",
      "shift num : 116\n",
      "shift num : 579\n",
      "shift num : 554\n",
      "shift num : 280\n",
      "shift num : 398\n",
      "shift num : 128\n",
      "shift num : 133\n",
      "shift num : 173\n",
      "shift num : 415\n",
      "# 완료!!\n",
      "##### ft_test 데이터 10 개 증강... #####\n",
      "# 완료!!\n",
      "RANDOM SEEDS RESET 1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a169d307ebac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                      verbose = 1, save_best_only = True)\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# model = my_dnn_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m#학습데이터 증강\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "idx = int(X_train.shape[-1] / 2) # 차분한 설명변수들을 구분하는 인덱스\n",
    "holdout_break = False\n",
    "aug_n = 10 # 증강 데이터 세트수\n",
    "\n",
    "final_pred_list = [] # 최종 예측값 리스트\n",
    "log_loss_list = [] # logloss 스코어 리스트\n",
    "\n",
    "# 테스트 데이터 증강\n",
    "reset_seeds(SEED)    \n",
    "X_test_aug = aug_data(X_test,\"X_test\" ,n=aug_n,shift=True,list_=True)\n",
    "ft_test_aug = aug_data(ft_test,\"ft_test\",n=aug_n,list_=True)\n",
    "\n",
    "model_idx = 0\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=0,shuffle=True)\n",
    "for tri, tei in kf.split(X_train,y):\n",
    "    reset_seeds(SEED)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    mc = ModelCheckpoint(f'best_model{model_idx}.h5', monitor = 'val_loss', mode = 'min', \n",
    "                     verbose = 1, save_best_only = True)\n",
    "    # model = my_dnn_model()\n",
    "    model\n",
    "    \n",
    "    #학습데이터 증강\n",
    "    X_tri = aug_data(X_train[tri],\"X_train[tri]\",n=aug_n,shift=True)\n",
    "    y_tri = aug_data(y[tri],\"y[tri]\",n=aug_n)\n",
    "    ft_train_tri = aug_data(ft_train[tri],\"ft_train[tri]\",n=aug_n)\n",
    "    tri_list = [ X_tri[:,:,:idx] , X_tri[:,:,idx:] ,  ft_train_tri ]\n",
    "    \n",
    "    #검증데이터는 증강 X\n",
    "    tei_list = [ X_train[tei,:,:idx] , X_train[tei,:,idx:] , ft_train[tei] ]\n",
    "    \n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        history = model.fit(tri_list, y_tri , epochs=100, batch_size=128,callbacks=[early_stop,mc],\n",
    "                            validation_data=(tei_list, y[tei]),\n",
    "                            )\n",
    "    \n",
    "    # 검증 데이터 증강후 각 세트를 예측하여 산술평균후에 logloss 값을 확인\n",
    "    reset_seeds(SEED)\n",
    "    X_tei_aug = aug_data(X_train[tei],\"X_train[tei]\",n=aug_n,shift=True,list_=True)\n",
    "    ft_train_tei_aug = aug_data(ft_train[tei],\"ft_train[tei]\",n=aug_n,list_=True)\n",
    "    aug_preds = []\n",
    "    loaded_model = load_model(f'best_model{model_idx}.h5') # 베스트 모델 로드\n",
    "    for X_aug,ft_aug in zip(X_tei_aug,ft_train_tei_aug):\n",
    "        aug_preds.append(\n",
    "            loaded_model.predict([ X_aug[:,:,:idx] , X_aug[:,:,idx:] ,ft_aug ])\n",
    "        )\n",
    "    aug_preds = np.mean(aug_preds,axis=0)\n",
    "    score = log_loss( y[tei] , aug_preds )\n",
    "    log_loss_list.append(score)\n",
    "    print(f\"log_loss : {score}\")\n",
    "    \n",
    "    # 증강된 테스트 데이터 각 세트를 예측하여 산술평균후 저장\n",
    "    aug_preds = []\n",
    "    for X_aug,ft_aug in zip(X_test_aug,ft_test_aug):\n",
    "        aug_preds.append(\n",
    "            loaded_model.predict([ X_aug[:,:,:idx] , X_aug[:,:,idx:] ,ft_aug ])\n",
    "        )\n",
    "    aug_preds = np.mean(aug_preds,axis=0)\n",
    "    final_pred_list.append(aug_preds)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_idx += 1\n",
    "    if holdout_break:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_loss mean: 0.5670415227912308\n",
      "log_loss std: 0.06273689175510266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5389355347841017,\n",
       " 0.5626589923185297,\n",
       " 0.4677849093673489,\n",
       " 0.6459557451124885,\n",
       " 0.6198724323736853]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"log_loss mean: {np.array(log_loss_list).mean()}\")\n",
    "print(f\"log_loss std: {np.array(log_loss_list).std()}\")\n",
    "final_pred = np.mean(final_pred_list,axis=0)\n",
    "log_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 예측파일 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3125</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>2.949999e-06</td>\n",
       "      <td>5.454564e-04</td>\n",
       "      <td>9.505763e-05</td>\n",
       "      <td>8.205664e-04</td>\n",
       "      <td>4.487088e-06</td>\n",
       "      <td>2.063583e-03</td>\n",
       "      <td>7.748483e-06</td>\n",
       "      <td>1.429730e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.455515e-03</td>\n",
       "      <td>9.858183e-04</td>\n",
       "      <td>3.187731e-04</td>\n",
       "      <td>1.422472e-04</td>\n",
       "      <td>3.740682e-04</td>\n",
       "      <td>3.997574e-06</td>\n",
       "      <td>9.240726e-08</td>\n",
       "      <td>8.820836e-03</td>\n",
       "      <td>2.654129e-06</td>\n",
       "      <td>8.349844e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3126</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>2.844041e-08</td>\n",
       "      <td>5.712100e-09</td>\n",
       "      <td>2.741548e-05</td>\n",
       "      <td>4.145272e-06</td>\n",
       "      <td>1.822872e-05</td>\n",
       "      <td>6.428497e-08</td>\n",
       "      <td>7.060175e-07</td>\n",
       "      <td>6.568233e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.431568e-08</td>\n",
       "      <td>1.870099e-10</td>\n",
       "      <td>1.665346e-07</td>\n",
       "      <td>8.111397e-08</td>\n",
       "      <td>9.185113e-07</td>\n",
       "      <td>5.942839e-08</td>\n",
       "      <td>4.254139e-08</td>\n",
       "      <td>1.043982e-07</td>\n",
       "      <td>1.151865e-08</td>\n",
       "      <td>4.035822e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3127</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>2.956220e-02</td>\n",
       "      <td>5.332585e-05</td>\n",
       "      <td>1.267966e-03</td>\n",
       "      <td>3.235794e-04</td>\n",
       "      <td>1.412775e-03</td>\n",
       "      <td>7.689697e-02</td>\n",
       "      <td>1.915085e-04</td>\n",
       "      <td>1.809220e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.021594e-04</td>\n",
       "      <td>3.923201e-05</td>\n",
       "      <td>5.857303e-05</td>\n",
       "      <td>3.292178e-03</td>\n",
       "      <td>4.637424e-05</td>\n",
       "      <td>4.203224e-04</td>\n",
       "      <td>7.404757e-07</td>\n",
       "      <td>2.774473e-04</td>\n",
       "      <td>4.495524e-03</td>\n",
       "      <td>3.666292e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3128</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>6.226859e-08</td>\n",
       "      <td>2.512742e-05</td>\n",
       "      <td>1.053882e-04</td>\n",
       "      <td>3.945113e-09</td>\n",
       "      <td>1.377589e-05</td>\n",
       "      <td>1.964818e-10</td>\n",
       "      <td>1.096983e-05</td>\n",
       "      <td>2.022974e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.561495e-08</td>\n",
       "      <td>9.028579e-09</td>\n",
       "      <td>2.420567e-07</td>\n",
       "      <td>1.945196e-06</td>\n",
       "      <td>1.174241e-04</td>\n",
       "      <td>5.800024e-07</td>\n",
       "      <td>2.547677e-05</td>\n",
       "      <td>1.284713e-07</td>\n",
       "      <td>3.553360e-10</td>\n",
       "      <td>1.274501e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3129</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>2.187010e-08</td>\n",
       "      <td>1.420652e-09</td>\n",
       "      <td>1.135435e-06</td>\n",
       "      <td>9.057795e-05</td>\n",
       "      <td>2.439244e-06</td>\n",
       "      <td>2.743650e-11</td>\n",
       "      <td>2.910950e-08</td>\n",
       "      <td>4.038291e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>8.178026e-11</td>\n",
       "      <td>4.656192e-12</td>\n",
       "      <td>1.370558e-09</td>\n",
       "      <td>1.270387e-11</td>\n",
       "      <td>4.026572e-09</td>\n",
       "      <td>1.740860e-09</td>\n",
       "      <td>6.382703e-05</td>\n",
       "      <td>4.002628e-08</td>\n",
       "      <td>3.543273e-09</td>\n",
       "      <td>3.630392e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>3902</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>6.777510e-08</td>\n",
       "      <td>4.310060e-09</td>\n",
       "      <td>2.600348e-05</td>\n",
       "      <td>3.111136e-03</td>\n",
       "      <td>9.632684e-06</td>\n",
       "      <td>2.342603e-09</td>\n",
       "      <td>4.804718e-07</td>\n",
       "      <td>2.345243e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.455163e-07</td>\n",
       "      <td>1.156058e-08</td>\n",
       "      <td>5.910046e-07</td>\n",
       "      <td>2.400488e-09</td>\n",
       "      <td>5.769946e-08</td>\n",
       "      <td>2.046839e-08</td>\n",
       "      <td>3.052598e-04</td>\n",
       "      <td>2.454884e-07</td>\n",
       "      <td>2.734131e-07</td>\n",
       "      <td>3.882496e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>3903</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>6.194850e-08</td>\n",
       "      <td>2.102190e-10</td>\n",
       "      <td>2.686587e-05</td>\n",
       "      <td>6.421692e-06</td>\n",
       "      <td>9.165403e-06</td>\n",
       "      <td>8.362011e-11</td>\n",
       "      <td>2.794029e-07</td>\n",
       "      <td>5.227568e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.781473e-10</td>\n",
       "      <td>3.312374e-12</td>\n",
       "      <td>4.619389e-08</td>\n",
       "      <td>1.884658e-10</td>\n",
       "      <td>4.821302e-09</td>\n",
       "      <td>3.378671e-08</td>\n",
       "      <td>2.713538e-06</td>\n",
       "      <td>1.097389e-08</td>\n",
       "      <td>2.122869e-09</td>\n",
       "      <td>4.362917e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>3904</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>3.483316e-10</td>\n",
       "      <td>5.987153e-09</td>\n",
       "      <td>1.752979e-06</td>\n",
       "      <td>1.410900e-05</td>\n",
       "      <td>2.548866e-07</td>\n",
       "      <td>1.765932e-10</td>\n",
       "      <td>4.484379e-08</td>\n",
       "      <td>1.617948e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>2.856205e-08</td>\n",
       "      <td>1.565856e-09</td>\n",
       "      <td>1.698061e-08</td>\n",
       "      <td>9.572945e-10</td>\n",
       "      <td>3.082543e-08</td>\n",
       "      <td>1.588561e-10</td>\n",
       "      <td>2.720129e-06</td>\n",
       "      <td>1.395063e-07</td>\n",
       "      <td>1.090977e-11</td>\n",
       "      <td>1.836785e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>3905</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>5.785850e-04</td>\n",
       "      <td>7.726508e-05</td>\n",
       "      <td>3.684409e-05</td>\n",
       "      <td>6.816952e-06</td>\n",
       "      <td>1.483243e-05</td>\n",
       "      <td>1.723245e-02</td>\n",
       "      <td>1.402121e-06</td>\n",
       "      <td>2.424638e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.175128e-05</td>\n",
       "      <td>7.907505e-06</td>\n",
       "      <td>3.401846e-06</td>\n",
       "      <td>8.223099e-04</td>\n",
       "      <td>5.526348e-05</td>\n",
       "      <td>1.416740e-04</td>\n",
       "      <td>2.487597e-08</td>\n",
       "      <td>2.797057e-05</td>\n",
       "      <td>5.737526e-04</td>\n",
       "      <td>4.763487e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>3906</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>5.250446e-08</td>\n",
       "      <td>4.182407e-09</td>\n",
       "      <td>3.674932e-07</td>\n",
       "      <td>7.373412e-06</td>\n",
       "      <td>5.519562e-06</td>\n",
       "      <td>1.094105e-11</td>\n",
       "      <td>2.816670e-06</td>\n",
       "      <td>3.819461e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070732e-09</td>\n",
       "      <td>2.162497e-09</td>\n",
       "      <td>2.838734e-08</td>\n",
       "      <td>8.995687e-10</td>\n",
       "      <td>2.298585e-09</td>\n",
       "      <td>3.291678e-09</td>\n",
       "      <td>4.332903e-04</td>\n",
       "      <td>1.004691e-05</td>\n",
       "      <td>5.860971e-10</td>\n",
       "      <td>6.105585e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         0             1             2             3             4  \\\n",
       "0    3125  0.000032  2.949999e-06  5.454564e-04  9.505763e-05  8.205664e-04   \n",
       "1    3126  0.000482  2.844041e-08  5.712100e-09  2.741548e-05  4.145272e-06   \n",
       "2    3127  0.000520  2.956220e-02  5.332585e-05  1.267966e-03  3.235794e-04   \n",
       "3    3128  0.000149  6.226859e-08  2.512742e-05  1.053882e-04  3.945113e-09   \n",
       "4    3129  0.000242  2.187010e-08  1.420652e-09  1.135435e-06  9.057795e-05   \n",
       "..    ...       ...           ...           ...           ...           ...   \n",
       "777  3902  0.002311  6.777510e-08  4.310060e-09  2.600348e-05  3.111136e-03   \n",
       "778  3903  0.000227  6.194850e-08  2.102190e-10  2.686587e-05  6.421692e-06   \n",
       "779  3904  0.000066  3.483316e-10  5.987153e-09  1.752979e-06  1.410900e-05   \n",
       "780  3905  0.000093  5.785850e-04  7.726508e-05  3.684409e-05  6.816952e-06   \n",
       "781  3906  0.000059  5.250446e-08  4.182407e-09  3.674932e-07  7.373412e-06   \n",
       "\n",
       "                5             6             7             8  ...  \\\n",
       "0    4.487088e-06  2.063583e-03  7.748483e-06  1.429730e-05  ...   \n",
       "1    1.822872e-05  6.428497e-08  7.060175e-07  6.568233e-06  ...   \n",
       "2    1.412775e-03  7.689697e-02  1.915085e-04  1.809220e-02  ...   \n",
       "3    1.377589e-05  1.964818e-10  1.096983e-05  2.022974e-07  ...   \n",
       "4    2.439244e-06  2.743650e-11  2.910950e-08  4.038291e-07  ...   \n",
       "..            ...           ...           ...           ...  ...   \n",
       "777  9.632684e-06  2.342603e-09  4.804718e-07  2.345243e-06  ...   \n",
       "778  9.165403e-06  8.362011e-11  2.794029e-07  5.227568e-07  ...   \n",
       "779  2.548866e-07  1.765932e-10  4.484379e-08  1.617948e-08  ...   \n",
       "780  1.483243e-05  1.723245e-02  1.402121e-06  2.424638e-06  ...   \n",
       "781  5.519562e-06  1.094105e-11  2.816670e-06  3.819461e-09  ...   \n",
       "\n",
       "               51            52            53            54            55  \\\n",
       "0    1.455515e-03  9.858183e-04  3.187731e-04  1.422472e-04  3.740682e-04   \n",
       "1    6.431568e-08  1.870099e-10  1.665346e-07  8.111397e-08  9.185113e-07   \n",
       "2    3.021594e-04  3.923201e-05  5.857303e-05  3.292178e-03  4.637424e-05   \n",
       "3    3.561495e-08  9.028579e-09  2.420567e-07  1.945196e-06  1.174241e-04   \n",
       "4    8.178026e-11  4.656192e-12  1.370558e-09  1.270387e-11  4.026572e-09   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "777  1.455163e-07  1.156058e-08  5.910046e-07  2.400488e-09  5.769946e-08   \n",
       "778  3.781473e-10  3.312374e-12  4.619389e-08  1.884658e-10  4.821302e-09   \n",
       "779  2.856205e-08  1.565856e-09  1.698061e-08  9.572945e-10  3.082543e-08   \n",
       "780  1.175128e-05  7.907505e-06  3.401846e-06  8.223099e-04  5.526348e-05   \n",
       "781  1.070732e-09  2.162497e-09  2.838734e-08  8.995687e-10  2.298585e-09   \n",
       "\n",
       "               56            57            58            59            60  \n",
       "0    3.997574e-06  9.240726e-08  8.820836e-03  2.654129e-06  8.349844e-06  \n",
       "1    5.942839e-08  4.254139e-08  1.043982e-07  1.151865e-08  4.035822e-06  \n",
       "2    4.203224e-04  7.404757e-07  2.774473e-04  4.495524e-03  3.666292e-04  \n",
       "3    5.800024e-07  2.547677e-05  1.284713e-07  3.553360e-10  1.274501e-04  \n",
       "4    1.740860e-09  6.382703e-05  4.002628e-08  3.543273e-09  3.630392e-05  \n",
       "..            ...           ...           ...           ...           ...  \n",
       "777  2.046839e-08  3.052598e-04  2.454884e-07  2.734131e-07  3.882496e-03  \n",
       "778  3.378671e-08  2.713538e-06  1.097389e-08  2.122869e-09  4.362917e-06  \n",
       "779  1.588561e-10  2.720129e-06  1.395063e-07  1.090977e-11  1.836785e-05  \n",
       "780  1.416740e-04  2.487597e-08  2.797057e-05  5.737526e-04  4.763487e-07  \n",
       "781  3.291678e-09  4.332903e-04  1.004691e-05  5.860971e-10  6.105585e-05  \n",
       "\n",
       "[782 rows x 62 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submit = submission.copy()\n",
    "final_submit.iloc[:,1:]=final_pred  \n",
    "final_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "끝!!\n"
     ]
    }
   ],
   "source": [
    "final_submit.to_csv(f'{SUB_PATH}submission_{np.array(log_loss_list).mean()}.csv', index=False)\n",
    "print(\"끝!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}