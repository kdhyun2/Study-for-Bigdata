{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 0. 라이브러리 및 설정\n",
    "# =============================================================================\n",
    "\n",
    "# 모듈 불러오기\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 경로 설정 및 customized function 불러오기\n",
    "project_dir = \"./.\"\n",
    "project_dir = os.path.abspath(project_dir)\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "from utils_.data_utils import convert_mdf\n",
    "from utils_ import arglist\n",
    "from utils_.Hierarchical_Convolutional_Attention_Regression import HConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. 데이터 리스트 로드\n",
    "# =============================================================================\n",
    "\n",
    "# 데이터 경로 설정 및 불러오기 (mdf 파일)\n",
    "data_dir = os.path.join(project_dir, 'data', arglist.file_dir)\n",
    "data_filenames = [f for f in os.listdir(data_dir) if f.endswith('.dat')]\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일별로 모델 생성, 결과 도출, 그래프 저장\n",
    "for data_filename in data_filenames:\n",
    "    \n",
    "    # 2.1 유니크 토큰 생성\n",
    "    # 여러 반복 실험을 하는 경우를 대비하여 현재 시간을 토큰으로 사용\n",
    "    unique_seed = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # 2.2 파라미터 설정\n",
    "    # arglist에서 기본 파라미터를 불러온 후, 수정하여 사용\n",
    "    param = arglist.PARAM()\n",
    "    param.data_filename = data_filename\n",
    "    param.network_type = 'HConv'\n",
    "\n",
    "    param.optimizer = tf.keras.optimizers.Adam()\n",
    "    param.scaler = StandardScaler(with_std=False)\n",
    "\n",
    "    param.n_attention = 10\n",
    "    param.embedding_dim = 20\n",
    "\n",
    "    param.segment_length = 20\n",
    "    param.segment_stride = 2\n",
    "\n",
    "    param.convert_log = True\n",
    "    param.weight_log = True\n",
    "\n",
    "    param.update_network_arguments()\n",
    "    assert param.updated\n",
    "    \n",
    "    # 2.3 데이터 로드\n",
    "    # mdf 데이터 불러오기\n",
    "    data = convert_mdf(mdf_filename=os.path.join(data_dir, data_filename),\n",
    "                       mdf_channels=param.mdf_channels,\n",
    "                       sample_frequency=0.01,\n",
    "                       save_file=False,\n",
    "                       csv_filename=None)\n",
    "    \n",
    "    # 2.4 데이터 전처리\n",
    "    # 목표량 = 100 - 목표량\n",
    "    data[arglist.valve_channels[1]] = -(data[arglist.valve_channels[1]] - 100)\n",
    "\n",
    "    data = data.drop(arglist.alarm_channels, axis=1)\n",
    "\n",
    "    data[arglist.valve_channels] = data[arglist.valve_channels].clip(lower=0)\n",
    "    egr_act = data[arglist.valve_channels[0]]\n",
    "    egr_r = data[arglist.valve_channels[1]]\n",
    "\n",
    "    # 타겟 변수를 만들기 위한 |목표값 - 실제값| 산출\n",
    "    egr_diff = np.abs(egr_act - egr_r).values\n",
    "\n",
    "    # 실제값은 분석에서 제외\n",
    "    data = data.drop(arglist.valve_channels[0], axis=1).values\n",
    "\n",
    "    # 데이터 스케일링: 평균만 0으로 맞춰주는 zero-centered scaling을 사용... param.scaler로 확인\n",
    "    if param.scaler is not None:\n",
    "        data = param.scaler.fit_transform(data)\n",
    "\n",
    "    # 슬라이딩 윈도우\n",
    "    X = []\n",
    "    y = []\n",
    "    window_start_index = []\n",
    "    for start_index in range(0, data.shape[0] - param.window_size, param.shift_size):\n",
    "        X.append(data[start_index:start_index + param.window_size])\n",
    "        y.append(egr_diff[start_index:start_index + param.window_size].sum())\n",
    "        window_start_index.append(start_index)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    window_start_index = np.asarray(window_start_index)\n",
    "    print('Total number of windows = {}'.format(len(X)))\n",
    "\n",
    "    # 학습 단계에서 가중치 부여\n",
    "    if param.weight_log:\n",
    "        y_weight = np.log(1 + y)\n",
    "    else:\n",
    "        y_weight = np.ones(shape=y.shape)\n",
    "\n",
    "    # 타겟 변수는 log transformation을 사용해서 학습\n",
    "    if param.convert_log:\n",
    "        y = np.log(1 + y)\n",
    "\n",
    "    # 학습:평가 = 8:2로 데이터셋 분리\n",
    "    X_train, X_test, y_train, y_test, y_weight_train, y_weight_test = train_test_split(\n",
    "        X, y, y_weight, test_size=0.2, random_state=2019, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = X\n",
    "train_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import chart_studio.plotly as ply\n",
    "import cufflinks as cf\n",
    "from pmdarima import auto_arima\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA_X(x_win):\n",
    "    \n",
    "    while True:\n",
    "        max_p = 3; max_d = 2; max_q = 3\n",
    "        results=[]\n",
    "\n",
    "        try:\n",
    "            par = auto_arima(x_win, start_p=0, start_q=0,d=0,\n",
    "                         test='adf',max_p=max_p, max_q=max_q,seasonal=False,\n",
    "                         trace=False, error_action='ignore')\n",
    "            order = par.order\n",
    "\n",
    "            model = ARIMA(x_win, order=(order[0], order[1], order[2]))\n",
    "            results = model.fit(disp=-1)\n",
    "            AR_model = results.arparams\n",
    "            MA_model = results.maparams\n",
    "            result_param = np.array([results.params[0]])\n",
    "\n",
    "            break\n",
    "        except (ValueError, np.linalg.LinAlgError):\n",
    "            try:\n",
    "                par = auto_arima(x_win, start_p=0, start_q=0,d=1,\n",
    "                         test='adf',max_p=max_p, max_q=max_q,seasonal=False,\n",
    "                         trace=False, error_action='ignore')\n",
    "                order = par.order\n",
    "\n",
    "                model = ARIMA(x_win, order=(order[0], order[1], order[2]))\n",
    "                results = model.fit(disp=-1)\n",
    "                AR_model = results.arparams\n",
    "                MA_model = results.maparams\n",
    "                result_param = np.array([results.params[0]])\n",
    "                break\n",
    "            except (ValueError, np.linalg.LinAlgError):\n",
    "                try:\n",
    "                    par = auto_arima(x_win, start_p=0, start_q=0,d=2,\n",
    "                         test='adf',max_p=max_p, max_q=max_q,seasonal=False,\n",
    "                         trace=False, error_action='ignore')\n",
    "                    order = par.order\n",
    "\n",
    "                    model = ARIMA(x_win, order=(order[0], order[1], order[2]))\n",
    "                    results = model.fit(disp=-1)\n",
    "                    AR_model = results.arparams\n",
    "                    MA_model = results.maparams\n",
    "                    result_param = np.array([results.params[0]])\n",
    "                    break\n",
    "                except (ValueError, np.linalg.LinAlgError):\n",
    "                    AR_model = np.array([])\n",
    "                    MA_model = np.array([])\n",
    "                    result_param = np.array([0])\n",
    "                    order = [0,0,0]\n",
    "                    break\n",
    "                    \n",
    "    opt_param = np.concatenate([AR_model, MA_model, result_param ])\n",
    "    \n",
    "    p_idx = np.arange(0, order[0], dtype=int)\n",
    "    q_idx = np.arange(order[0], order[0]+order[2], dtype=int)\n",
    "\n",
    "    if len(p_idx)==0:\n",
    "        AR = np.r_[np.zeros(max_p-np.size(p_idx))]\n",
    "    else:\n",
    "        AR = np.r_[opt_param[0:len(p_idx)], np.zeros(max_p-np.size(p_idx))]\n",
    "\n",
    "    if len(q_idx)==0:\n",
    "        MA = np.r_[np.zeros(max_q-np.size(q_idx))]\n",
    "    else:\n",
    "        MA = np.r_[opt_param[len(p_idx):len(p_idx)+len(q_idx)], np.zeros(max_q-np.size(q_idx))]\n",
    "    \n",
    "    return np.concatenate([AR, MA ,np.array([opt_param[-1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = []\n",
    "for i in range(train_dat.shape[0]):\n",
    "    arima_win = []\n",
    "    for j in range(train_dat.shape[2]):\n",
    "        aim = ARIMA_X(train_dat[i,:,j])\n",
    "        arima_win.append(aim)\n",
    "    arima_win = np.reshape(arima_win, (1, np.size(arima_win)))\n",
    "    arima.append(arima_win)\n",
    "    print(i)\n",
    "\n",
    "arima = np.array(arima, dtype=np.float32)\n",
    "arima = np.reshape(arima, (arima.shape[0], arima.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/arima.csv', X=arima, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywt import wavedec, Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WT(x_win):\n",
    "    \n",
    "    w = Wavelet('sym4')\n",
    "    cA3, cD4, cD3, cD2, cD1 = wavedec(x_win, w)\n",
    "    \n",
    "    plf = np.polyfit(np.arange(np.size(cA3)), cA3, 5)\n",
    "    cD1 = np.std(cD1)\n",
    "    cD2 = np.std(cD2)\n",
    "    cD3 = np.std(cD3)\n",
    "    cD4 = np.std(cD4)\n",
    "    \n",
    "    return np.r_[plf, cD1, cD2, cD3, cD4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelets = []\n",
    "for i in range(train_dat.shape[0]):\n",
    "    wt_win = []\n",
    "    for j in range(train_dat.shape[2]):\n",
    "        wts = WT(train_dat[i,:,j])\n",
    "        wt_win.append(wts)\n",
    "    wt_win = np.reshape(wt_win, (1, np.size(wt_win)))\n",
    "    wavelets.append(wt_win)\n",
    "    print(i)\n",
    "\n",
    "wavelets = np.array(wavelets, dtype=np.float32)\n",
    "wavelets = np.reshape(wavelets, (wavelets.shape[0], wavelets.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/wavelets.csv', X=wavelets, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from dbn.models import UnsupervisedDBN\n",
    "# pip install git\n",
    "# CPU : pip install git+git://github.com/albertbup/deep-belief-network.git\n",
    "# GPU : pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBN(x_win):\n",
    "    dbn = UnsupervisedDBN(hidden_layers_structure=[50, 30, 20],\n",
    "                      batch_size=100,\n",
    "                      learning_rate_rbm=0.05,\n",
    "                      n_epochs_rbm=100,\n",
    "                      activation_function='sigmoid')\n",
    "    dbn.fit(x_win)\n",
    "    return dbn.transform(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbns = []\n",
    "for i in range(train_dat.shape[0]):\n",
    "    dbn_win = []\n",
    "    \n",
    "    dbs = DBN(train_dat[i,:,:])\n",
    "    dbn_win.append(dbs)\n",
    "    \n",
    "    dbn_win = np.reshape(dbn_win, (1, np.size(dbn_win)))\n",
    "    dbns.append(dbn_win)\n",
    "\n",
    "dbns = np.array(dbns, dtype=np.float32)\n",
    "dbns = np.reshape(dbns, (dbns.shape[0], dbns.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/dbns.csv', X=dbns, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "import os\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleAE(x_win):\n",
    "    input_dim = x_win.shape[1]\n",
    "    encoding_dim = 50\n",
    "    \n",
    "    autoencoder = Sequential()\n",
    "    autoencoder.add(Dense(encoding_dim, input_shape=(input_dim,), activation='relu'))\n",
    "    autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "    autoencoder.fit(x_win,x_win,\n",
    "                        epochs=100,\n",
    "                        batch_size=100,\n",
    "                        shuffle=True, verbose=0)\n",
    "    ##########################################\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "\n",
    "    encoder_layer = autoencoder.layers[0]\n",
    "    encoder = Model(input_img, encoder_layer(input_img))\n",
    "    \n",
    "    return encoder.predict(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simple_ae = []\n",
    "\n",
    "for i in range(518,train_dat.shape[0]):\n",
    "    simple_ae_win = []\n",
    "    \n",
    "    sae = simpleAE(train_dat[i,:,:])\n",
    "    simple_ae_win.append(sae)\n",
    "    \n",
    "    simple_ae_win = np.reshape(simple_ae_win, (1, np.size(simple_ae_win)))\n",
    "    simple_ae.append(simple_ae_win)\n",
    "    print(i)\n",
    "    \n",
    "\n",
    "simple_ae = np.array(simple_ae, dtype=np.float32)\n",
    "simple_ae = np.reshape(simple_ae, (simple_ae.shape[0], simple_ae.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ae = np.array(simple_ae, dtype=np.float32)\n",
    "simple_ae = np.reshape(simple_ae, (simple_ae.shape[0], simple_ae.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/simple_ae(518_).csv', X=simple_ae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackedAE(x_win):\n",
    "    input_dim = x_win.shape[1]\n",
    "    encoding_dim = 50\n",
    "    \n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    # Encoder Layers\n",
    "    autoencoder.add(Dense(2 * encoding_dim, input_shape=(input_dim,), activation='relu'))\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "\n",
    "    # Decoder Layers\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(x_win, x_win,\n",
    "                epochs=100,\n",
    "                batch_size=100, verbose=0)\n",
    "###################################################################################################\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = autoencoder.layers[0]\n",
    "    encoder_layer2 = autoencoder.layers[1]\n",
    "    encoder_layer3 = autoencoder.layers[2]\n",
    "    encoder = Model(input_img, encoder_layer3(encoder_layer2(encoder_layer1(input_img))))\n",
    "\n",
    "    return encoder.predict(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_ae = []\n",
    "\n",
    "for i in range(train_dat.shape[0]):\n",
    "    stack_ae_win = []\n",
    "    \n",
    "    sae = stackedAE(train_dat[i,:,:])\n",
    "    stack_ae_win.append(sae)\n",
    "    \n",
    "    stack_ae_win = np.reshape(stack_ae_win, (1, np.size(stack_ae_win)))\n",
    "    stack_ae.append(stack_ae_win)\n",
    "    print(i)\n",
    "    \n",
    "\n",
    "stack_ae = np.array(stack_ae, dtype=np.float32)\n",
    "stack_ae = np.reshape(stack_ae, (stack_ae.shape[0], stack_ae.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/stack_ae.csv', X=stack_ae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deAE(x_win):\n",
    "    \n",
    "    input_dim = x_win.shape[1]\n",
    "    encoding_dim = 50\n",
    "    \n",
    "    noise_factor = 0.5\n",
    "    x_win_noisy = x_win + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_win.shape) \n",
    "    x_win_noisy = np.clip(x_win_noisy, 0., 1.)\n",
    "\n",
    "\n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    # Encoder Layers\n",
    "    autoencoder.add(Dense(2 * encoding_dim, input_shape=(input_dim,), activation='relu'))\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "\n",
    "    # Decoder Layers\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(2 * encoding_dim, activation='relu'))\n",
    "    autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(x_win_noisy, x_win,\n",
    "                epochs=100,\n",
    "                batch_size=100, verbose=0)\n",
    "\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoder_layer1 = autoencoder.layers[0]\n",
    "    encoder_layer2 = autoencoder.layers[1]\n",
    "    encoder_layer3 = autoencoder.layers[2]\n",
    "    encoder = Model(input_img, encoder_layer3(encoder_layer2(encoder_layer1(input_img))))\n",
    "\n",
    "    return encoder.predict(x_win_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_ae = []\n",
    "\n",
    "for i in range(train_dat.shape[0]):\n",
    "    de_ae_win = []\n",
    "    \n",
    "    dae_return = deAE(train_dat[i,:,:])\n",
    "    de_ae_win.append(dae_return)\n",
    "    \n",
    "    de_ae_win = np.reshape(de_ae_win, (1, np.size(de_ae_win)))\n",
    "    de_ae.append(de_ae_win)\n",
    "    print(i)\n",
    "\n",
    "de_ae = np.array(de_ae, dtype=np.float32)\n",
    "de_ae = np.reshape(de_ae, (de_ae.shape[0], de_ae.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/de_ae.csv', X=de_ae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Variational Autoencoder(VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch,dim))\n",
    "    return z_mean +K.exp(0.5*z_log_var)*epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAE(x_win):\n",
    "    \n",
    "    input_dim = x_win.shape[1]\n",
    "\n",
    "    inputs = Input(shape=(input_dim,),name='encoder_input')\n",
    "\n",
    "    x = Dense(256,activation='relu')(inputs)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    z_mean = Dense(2,name='z_mean')(x)\n",
    "    z_log_var = Dense(2,name='z_log_var')(x)\n",
    "    \n",
    "    z = Lambda(sampling,output_shape=(2,),name='z')([z_mean,z_log_var])\n",
    "\n",
    "    encoder = Model(inputs,[z_mean,z_log_var,z],name='encoder')\n",
    "#####################################################################################\n",
    "    latent_inputs = Input(shape=(2,),name='z_sampling')\n",
    "    x = Dense(256,activation='relu')(latent_inputs)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    outputs = Dense(input_dim,activation='sigmoid')(x) # 0~1\n",
    "    \n",
    "    decoder = Model(latent_inputs,outputs,name='decoder')\n",
    "#####################################################################################\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = Model(inputs,outputs,name='vae_mlp')\n",
    "\n",
    "    models = (encoder,decoder)\n",
    "    \n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = -0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "\n",
    "    vae.compile(optimizer='adam',loss=vae_loss)\n",
    "\n",
    "    vae.fit(x_win,x_win,epochs=100,batch_size=100, verbose=0)\n",
    "#####################################################################################\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "\n",
    "    encoder_layer1 = vae.layers[0]\n",
    "    encoder_layer2 = vae.layers[1]\n",
    "\n",
    "    encoder2 = Model(input_img, encoder_layer2(encoder_layer1(input_img)))\n",
    "\n",
    "    return encoder2.predict(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = []\n",
    "\n",
    "for i in range(train_dat.shape[0]):\n",
    "    vae_win = []\n",
    "    \n",
    "    vae_return = VAE(train_dat[i,:,:])\n",
    "    vae_win.append(vae_return)\n",
    "    \n",
    "    vae_win = np.reshape(vae_win, (1, np.size(vae_win)))\n",
    "    vae.append(vae_win)\n",
    "    print(i)\n",
    "\n",
    "vae = np.array(vae, dtype=np.float32)\n",
    "vae = np.reshape(vae, (vae.shape[0], vae.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/vae.csv', X=vae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Convolutional Autoencoder(CAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAE(x_win):\n",
    "    \n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    # Encoder Layers\n",
    "    autoencoder.add(Conv1D(26, 3, activation='relu', padding='same', input_shape=x_win.shape[1:]))\n",
    "    autoencoder.add(MaxPooling1D(5, padding='same'))\n",
    "    autoencoder.add(Conv1D(13, 3, activation='relu', padding='same'))\n",
    "    autoencoder.add(MaxPooling1D(5, padding='same'))\n",
    "    autoencoder.add(Conv1D(5, 3, activation='relu', padding='same'))\n",
    "\n",
    "    # Flatten encoding for visualization\n",
    "    autoencoder.add(Flatten())\n",
    "    autoencoder.add(Reshape((8, 5)))\n",
    "    autoencoder.add(Conv1D(5, 3, activation='relu', padding='same'))\n",
    "    autoencoder.add(UpSampling1D(5))\n",
    "    autoencoder.add(Conv1D(13, 3, activation='relu', padding='same'))\n",
    "    autoencoder.add(UpSampling1D(5))\n",
    "    autoencoder.add(Conv1D(26, 3, activation='relu', padding='same'))\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(x_win, x_win,\n",
    "                epochs=100,\n",
    "                batch_size=100, verbose=0)\n",
    "##################################################################################################\n",
    "    input_img = Input(shape=x_win.shape[1:])\n",
    "    encoder_layer1 = autoencoder.layers[0]\n",
    "    encoder_layer2 = autoencoder.layers[1]\n",
    "    encoder_layer3 = autoencoder.layers[2]\n",
    "    encoder_layer4 = autoencoder.layers[3]\n",
    "    encoder_layer5 = autoencoder.layers[4]\n",
    "    encoder = Model(input_img, encoder_layer5(encoder_layer4(encoder_layer3(encoder_layer2(encoder_layer1(input_img))))))\n",
    "    return encoder.predict(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for batch training (slicing traing set into proper batches)\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(train_dat)\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.arange(n_train)\n",
    "train_batch = list(chunks(train_idx, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cae = []\n",
    "cae_win = []\n",
    "\n",
    "for batch_idx in train_batch:\n",
    "    cae_return = CAE(train_dat[batch_idx])\n",
    "    \n",
    "    for i in range(cae_return.shape[0]):\n",
    "        cae_win.append(cae_return[i])\n",
    "    print(batch_idx[0])\n",
    "\n",
    "cae_win = np.reshape(cae_win, (train_dat.shape[0], int(np.size(cae_win)/train_dat.shape[0])))\n",
    "cae = np.array(cae_win, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/cae.csv', X=cae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Lambda, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_AE(x_win):\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(x_win.shape[1:])))\n",
    "    model.add(RepeatVector(x_win.shape[1]))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(x_win.shape[2])))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(x_win, x_win, epochs=100, verbose=0)\n",
    "#################################################################################\n",
    "    input_img = Input(shape=(x_win.shape[1:]))\n",
    "\n",
    "    encoder_layer1 = model.layers[0]\n",
    "    encoder_layer2 = model.layers[1]\n",
    "\n",
    "    encoder = Model(input_img, encoder_layer2(encoder_layer1(input_img)))\n",
    "    return encoder.predict(x_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_ae = []\n",
    "lstm_ae_win = []\n",
    "\n",
    "for batch_idx in train_batch:\n",
    "    lae_return = LSTM_AE(train_dat[batch_idx])\n",
    "    \n",
    "    for i in range(lae_return.shape[0]):\n",
    "        lstm_ae_win.append(lae_return[i])\n",
    "    print(batch_idx[0])\n",
    "\n",
    "lstm_ae_win = np.reshape(lstm_ae_win, (train_dat.shape[0], int(np.size(lstm_ae_win)/train_dat.shape[0])))\n",
    "lstm_ae = np.array(lstm_ae_win, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('c:/tf2/lstm_ae.csv', X=lstm_ae, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering as HC\n",
    "from sklearn.metrics import silhouette_score as ss\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "from mvpa2.mappers.som import SimpleSOMMapper\n",
    "from sklearn.preprocessing import OneHotEncoder as ENC\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = np.loadtxt(fname='c:/feature/fin/arima.csv', delimiter=',', skiprows=0)\n",
    "arima = np.array(arima)\n",
    "arima = np.nan_to_num(arima)\n",
    "\n",
    "wavelets = np.loadtxt(fname='c:/feature/fin/wavelets.csv', delimiter=',', skiprows=0)\n",
    "wavelets = np.array(wavelets)\n",
    "wavelets = np.nan_to_num(wavelets)\n",
    "\n",
    "dbns = np.loadtxt(fname='c:/feature/fin/dbns.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "simple_ae = np.loadtxt(fname='c:/feature/fin/simple_ae.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "stack_ae = np.loadtxt(fname='c:/feature/fin/stack_ae.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "de_ae = np.loadtxt(fname='c:/feature/fin/de_ae.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "vae = np.loadtxt(fname='c:/feature/fin/vae.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "cae = np.loadtxt(fname='c:/feature/fin/cae.csv', delimiter=',', skiprows=0)\n",
    "\n",
    "lstm_ae = np.loadtxt(fname='c:/feature/fin/lstm_ae.csv', delimiter=',', skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [arima,wavelets,dbns,simple_ae,stack_ae,de_ae,vae,cae,lstm_ae]\n",
    "data2 = ['arima','wavelets','dbns','simple_ae','stack_ae','de_ae','vae','cae','lstm_ae']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_cluster(feature):\n",
    "    cluster_size = range(5, 21)\n",
    "    scores, idx_tmp = [], []\n",
    "    for n_cluster in cluster_size:\n",
    "        hc = HC(n_clusters=n_cluster, linkage='ward')\n",
    "        hc_idx = hc.fit_predict(feature)\n",
    "        idx_tmp.append(hc_idx)\n",
    "        scores.append(ss(feature, hc_idx))\n",
    "    cluster_labels = idx_tmp[np.argmax(scores)]\n",
    "    \n",
    "    df = pd.DataFrame({'cluster' : cluster_labels, 'abnormal' : abnormal_idx}, columns=['cluster', 'abnormal'])\n",
    "    \n",
    "    ab = []\n",
    "    nor = []\n",
    "    \n",
    "    length = len(pd.Series.value_counts(cluster_labels))\n",
    "\n",
    "    for i in range(0,length):\n",
    "        ab_i = np.sum(df[df['cluster']==i]['abnormal'])\n",
    "        nor_i = len(df[df['cluster']==i]['abnormal'])-ab_i\n",
    "        ab.append(ab_i)\n",
    "        nor.append(nor_i)\n",
    "\n",
    "    return (ab, nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_abnormal = []\n",
    "h_normal = []\n",
    "h = []\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    h = h_cluster(data[i])\n",
    "    h_abnormal.append(h[0])\n",
    "    h_normal.append(h[1])\n",
    "    print(data2[i],'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_result = []\n",
    "\n",
    "for j in range(0,len(data)):\n",
    "    a = np.divide(np.array(h_normal[j]), np.array(h_abnormal[j])+np.array(h_normal[j]))\n",
    "    b = np.divide(np.array(h_abnormal[j]), np.array(h_abnormal[j])+np.array(h_normal[j]))\n",
    "\n",
    "    c = sum(h_normal[j])+sum(h_abnormal[j])\n",
    "    d = np.array(h_normal[j])+np.array(h_abnormal[j])\n",
    "    result_data = sum(entropy([a,b],base=2) * d/c)\n",
    "    \n",
    "    h_result.append(round(result_data,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame((data2,h_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Self-Organizing Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def som_cluster(feature):\n",
    "    cluster_size = range(5, 21)\n",
    "    scores, idx_tmp = [], []\n",
    "    for n_cluster in cluster_size:\n",
    "        som = SimpleSOMMapper((n_cluster, n_cluster), learning_rate=0.02, niter=500)\n",
    "        som.train(feature)\n",
    "        encoder = ENC(sparse=False)\n",
    "        tmp = som.forward(feature)\n",
    "        som_idx = np.argmax(encoder.fit_transform((tmp[:,0]+tmp[:,1]**2).reshape(len(feature),1)), axis=1)\n",
    "        scores.append(ss(feature, som_idx))\n",
    "        idx_tmp.append(som_idx)\n",
    "\n",
    "    cluster_labels = idx_tmp[np.argmax(scores)]\n",
    "    \n",
    "    df = pd.DataFrame({'cluster' : cluster_labels, 'abnormal' : abnormal_idx}, columns=['cluster', 'abnormal'])\n",
    "    \n",
    "    length = len(pd.Series.value_counts(cluster_labels))\n",
    "\n",
    "    ab = []\n",
    "    nor = []\n",
    "\n",
    "    for i in range(0,length):\n",
    "        ab_i = np.sum(df[df['cluster']==i]['abnormal'])\n",
    "        nor_i = len(df[df['cluster']==i]['abnormal'])-ab_i\n",
    "        ab.append(ab_i)\n",
    "        nor.append(nor_i)\n",
    "\n",
    "    return (ab, nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_abnormal = []\n",
    "som_normal = []\n",
    "som = []\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    som = som_cluster(data[i])\n",
    "    som_abnormal.append(som[0])\n",
    "    som_normal.append(som[1])\n",
    "    print(data2[i],'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_result = []\n",
    "\n",
    "for j in range(0,len(data)):\n",
    "    a = np.divide(np.array(som_normal[j]), np.array(som_abnormal[j])+np.array(som_normal[j]))\n",
    "    b = np.divide(np.array(som_abnormal[j]), np.array(som_abnormal[j])+np.array(som_normal[j]))\n",
    "\n",
    "    c = sum(som_normal[j])+sum(som_abnormal[j])\n",
    "    d = np.array(som_normal[j])+np.array(som_abnormal[j])\n",
    "    result_data = sum(entropy([a,b],base=2) * d/c)\n",
    "    \n",
    "    som_result.append(round(result_data,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM_cluster(feature):\n",
    "\n",
    "    cluster_size = range(5, 21)\n",
    "    scores, idx_tmp = [], []\n",
    "    for n_cluster in cluster_size:\n",
    "        KM = KMeans(n_clusters=n_cluster,algorithm='auto')\n",
    "        KM_idx = KM.fit_predict(feature)\n",
    "        scores.append(ss(feature, KM_idx))\n",
    "        idx_tmp.append(KM_idx)\n",
    "\n",
    "    cluster_labels = idx_tmp[np.argmax(scores)]\n",
    "    \n",
    "    df = pd.DataFrame({'cluster' : cluster_labels, 'abnormal' : abnormal_idx}, columns=['cluster', 'abnormal'])\n",
    "    \n",
    "    length = len(pd.Series.value_counts(cluster_labels))\n",
    "\n",
    "    ab = []\n",
    "    nor = []\n",
    "\n",
    "    for i in range(0,length):\n",
    "        ab_i = np.sum(df[df['cluster']==i]['abnormal'])\n",
    "        nor_i = len(df[df['cluster']==i]['abnormal'])-ab_i\n",
    "        ab.append(ab_i)\n",
    "        nor.append(nor_i)\n",
    "\n",
    "    return (ab, nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_abnormal = []\n",
    "KM_normal = []\n",
    "KM = []\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    KM = KM_cluster(data[i])\n",
    "    KM_abnormal.append(KM[0])\n",
    "    KM_normal.append(KM[1])\n",
    "    print(data2[i],'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_result = []\n",
    "\n",
    "for j in range(0,len(data)):\n",
    "    a = np.divide(np.array(KM_normal[j]), np.array(KM_abnormal[j])+np.array(KM_normal[j]))\n",
    "    b = np.divide(np.array(KM_abnormal[j]), np.array(KM_abnormal[j])+np.array(KM_normal[j]))\n",
    "\n",
    "    c = sum(KM_normal[j])+sum(KM_abnormal[j])\n",
    "    d = np.array(KM_normal[j])+np.array(KM_abnormal[j])\n",
    "    result_data = sum(entropy([a,b],base=2) * d/c)\n",
    "    \n",
    "    KM_result.append(round(result_data,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(KM_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame((data2,KM_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
