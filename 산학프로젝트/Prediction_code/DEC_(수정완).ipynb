{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEC.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv1DBiyT_5OP"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/data/real_data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FZwuTo3iix9"
      },
      "source": [
        "from src.utils import AbstractBaseModel\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.losses import kld\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdMfRxtkis8Z"
      },
      "source": [
        "class ClusteringLayer(Layer):\n",
        "    \n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
        "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
        "        Arguments:\n",
        "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        Return:\n",
        "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        \"\"\"\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpP-EiNHjZqM"
      },
      "source": [
        "def ConvBNActBlock(x, activation, filter_size, filter_dim, kernel_initializer, kernel_regularizer, name=None):\n",
        "    x = Conv2D(filters=filter_dim,\n",
        "               kernel_size=filter_size,\n",
        "               padding='same',\n",
        "               kernel_initializer=kernel_initializer,\n",
        "               kernel_regularizer=kernel_regularizer,\n",
        "               name=f'{name}-conv')(x)\n",
        "    x = BatchNormalization(name=f'{name}-bn')(x)\n",
        "    x = Activation(activation, name=f'{name}-act')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def ConvTrBNActBlock(x, activation, filter_size, filter_dim, kernel_initializer, kernel_regularizer, name=None):\n",
        "    x = Conv2DTranspose(filters=filter_dim,\n",
        "                        kernel_size=filter_size,\n",
        "                        padding='same',\n",
        "                        kernel_initializer=kernel_initializer,\n",
        "                        kernel_regularizer=kernel_regularizer,\n",
        "                        name=f'{name}-convtr')(x)\n",
        "    x = BatchNormalization(name=f'{name}-bn')(x)\n",
        "    x = Activation(activation, name=f'{name}-act')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def BottleneckBlock(x, activation, embedding_dim, kernel_initializer, kernel_regularizer, name=None):\n",
        "    _raw_shape = x.shape[1:]\n",
        "    x = Flatten(name=f'{name}-flatten')(x)\n",
        "    _flat_shape = x.shape[1]\n",
        "    \n",
        "    h = Dense(units=embedding_dim,\n",
        "              kernel_initializer=kernel_initializer,\n",
        "              kernel_regularizer=kernel_regularizer,\n",
        "              name=f'{name}-embedding')(x)\n",
        "    x = BatchNormalization(name=f'{name}-bn1')(h)\n",
        "    x = Activation(activation, name=f'{name}-act1')(x)\n",
        "    \n",
        "    x = Dense(units=_flat_shape,\n",
        "              kernel_initializer=kernel_initializer,\n",
        "              kernel_regularizer=kernel_regularizer,\n",
        "              name=f'{name}-dense')(x)\n",
        "    x = Reshape(_raw_shape, name=f'{name}-reshape')(x)\n",
        "    x = BatchNormalization(name=f'{name}-bn2')(x)\n",
        "    x = Activation(activation, name=f'{name}-act2')(x)\n",
        "    \n",
        "    return h, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnVIkEkVjgLI"
      },
      "source": [
        "class DEC(object):\n",
        "    def __init__(self, \n",
        "                 n_clusters='auto',\n",
        "                 window_size=8, num_channels=3,\n",
        "                 filter_size=20, filter_dim=50,\n",
        "                 embedding_dim=64,\n",
        "                 alpha=1.0,\n",
        "                 kernel_initializer='glorot_normal',\n",
        "                 kernel_regularizer=None):\n",
        "        \n",
        "        self._ae, self._enc = self._define_model(\n",
        "                window_size, num_channels,\n",
        "                filter_size, filter_dim,\n",
        "                embedding_dim,\n",
        "                kernel_initializer,\n",
        "                kernel_regularizer)\n",
        "        \n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.pretrained = False\n",
        "        \n",
        "        if type(n_clusters) == int:\n",
        "            cluster = ClusteringLayer(self.n_clusters, name='clustering')(self._enc.output)\n",
        "            self.cluster = Model(self._enc.input, cluster)\n",
        "        \n",
        "\n",
        "    def train(self, x, maxiter=2000, batch_size=128, tol=1E-3,\n",
        "              update_interval=250):\n",
        "\n",
        "        if not self.pretrained:\n",
        "            self.pretrain(x)\n",
        "            \n",
        "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
        "        y_pred = kmeans.fit_predict(self._enc.predict(x))\n",
        "        y_pred_last = np.copy(y_pred)\n",
        "        \n",
        "        self.cluster.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
        "        sgd = keras.optimizers.SGD(lr = 0.001)\n",
        "        self.cluster.compile(optimizer= sgd, loss='kld', metrics=['accuracy'])\n",
        "        #self.cluster.compile(optimizer='SGD', loss='kld', metrics=['accuracy'], learning_rate=learning_rate)\n",
        "\n",
        "        loss = 0\n",
        "        index = 0\n",
        "        index_array = np.arange(x.shape[0])\n",
        "        for i in range(maxiter):\n",
        "            if i % update_interval == 0:\n",
        "                q = self.cluster.predict(x)\n",
        "                p = self.target_distribution(q)\n",
        "\n",
        "                y_pred = q.argmax(1)\n",
        "\n",
        "                delta_label = np.mean(y_pred != y_pred_last).astype(np.float32)\n",
        "                y_pred_last = np.copy(y_pred)\n",
        "                \n",
        "                if i > 0 and delta_label < tol:\n",
        "                    print('delta_label ', delta_label, '< tol ', tol)\n",
        "                    print('Reached tolerance threshold. Stopping training.')\n",
        "                    break\n",
        "                \n",
        "                np.random.shuffle(index_array)\n",
        "                \n",
        "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "            loss = self.cluster.train_on_batch(x=x[idx], y=p[idx])\n",
        "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
        "\n",
        "\n",
        "    def autofit(self, x, maxiter=2000, batch_size=128, learning_rate=1E-3, tol=1E-3,\n",
        "                update_interval=200):\n",
        "        \n",
        "        train_x, valid_x = train_test_split(x, test_size=0.2)\n",
        "        self._train_ae(train_x)\n",
        "        \n",
        "        n_clusters_range = list(range(2, 11))\n",
        "        loss_ratio = []\n",
        "        for n_clusters in n_clusters_range:\n",
        "            cluster = ClusteringLayer(n_clusters, name='clustering')(self.enc.output)\n",
        "            self.cluster = Model(self.enc.input, cluster)\n",
        "            self.train(train_x, maxiter=maxiter, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                       tol=tol, update_interval=update_interval)\n",
        "            \n",
        "            train_q = self.cluster.predict(train_x)\n",
        "            train_p = self.target_distribution(train_q)\n",
        "            \n",
        "            train_loss = kld(train_p, train_x)\n",
        "            \n",
        "            valid_q = self.cluster.predict(valid_x)\n",
        "            valid_p = self.target_distribution(valid_q)\n",
        "            \n",
        "            valid_loss = kld(valid_p, valid_x)\n",
        "            \n",
        "            loss_ratio.append(train_loss / valid_loss)\n",
        "        \n",
        "        self.n_clusters = n_clusters_range[np.argmax(np.diff(loss_ratio)) + 1]\n",
        "        cluster = ClusteringLayer(self.n_clusters, name='clustering')(self.enc.output)\n",
        "        self.cluster = Model(self.enc.input, cluster)\n",
        "        self.train(x)\n",
        "        \n",
        "    def predict(self, x):\n",
        "        q = self.cluster.predict(x)\n",
        "        return np.argmax(q, 1)\n",
        "    \n",
        "    def _get_embedding(self, x):\n",
        "        z = self._enc.predict(x)\n",
        "        return z\n",
        "        \n",
        "    def _get_recon_error(self, x):\n",
        "        _x = self._ae.predict(x)\n",
        "        recon_error = np.square(x - _x).mean((1, 2))\n",
        "        return recon_error\n",
        "    \n",
        "    def pretrain(self, x, epochs=200, batch_size=128):\n",
        "        self._ae.compile(optimizer='Adam', loss='mse', metrics=['mse'])\n",
        "        self._ae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
        "    ])\n",
        "        self.pretrained = True\n",
        "\n",
        "    @staticmethod\n",
        "    def target_distribution(q):\n",
        "        weight = q ** 2 / q.sum(0)\n",
        "        return (weight.T / weight.sum(1)).T\n",
        "    \n",
        "    \n",
        "    def _define_model(self, window_size, num_channels,\n",
        "                      filter_size, filter_dim,\n",
        "                      embedding_dim,\n",
        "                      kernel_initializer,\n",
        "                      kernel_regularizer,\n",
        "                      activation='relu',\n",
        "                      output_act=None):\n",
        "        \n",
        "        filter_size = (filter_size, 1)\n",
        "        \n",
        "        conv_block_args = {\n",
        "                'activation': activation,\n",
        "                'filter_size': filter_size,\n",
        "                'kernel_initializer': kernel_initializer,\n",
        "                'kernel_regularizer': kernel_regularizer\n",
        "                }\n",
        "        \n",
        "        _x_in = Input(shape=(window_size, 1, num_channels), name='input')\n",
        "        \n",
        "        x = ConvBNActBlock(_x_in, name='block1', filter_dim=filter_dim, **conv_block_args)\n",
        "        x = AveragePooling2D(pool_size=(2, 1), name='pool1')(x)\n",
        "        \n",
        "        x = ConvBNActBlock(x, name='block2', filter_dim=filter_dim*2, **conv_block_args)\n",
        "        x = AveragePooling2D(pool_size=(2, 1), name='pool2')(x)\n",
        "        \n",
        "        x = ConvBNActBlock(x, name='block3', filter_dim=filter_dim*4, **conv_block_args)\n",
        "        \n",
        "        _z_out, x = BottleneckBlock(x, activation, embedding_dim, kernel_initializer,\n",
        "                                    kernel_regularizer, name='blottleneck')\n",
        "        \n",
        "        x = UpSampling2D(size=(2, 1), name='upsample1')(x)\n",
        "        x = ConvTrBNActBlock(x, name='block4', filter_dim=filter_dim*2, **conv_block_args)\n",
        "        \n",
        "        x = UpSampling2D(size=(2, 1), name='upsample2')(x)\n",
        "        x = ConvTrBNActBlock(x, name='block5', filter_dim=filter_dim, **conv_block_args)\n",
        "        \n",
        "        x = Conv2DTranspose(filters=num_channels, kernel_size=filter_size, padding='same',\n",
        "                            kernel_initializer=kernel_initializer,\n",
        "                            kernel_regularizer=kernel_regularizer,\n",
        "                            name='output-convtr')(x)\n",
        "        _x_out = Activation(activation=output_act, name='output-act')(x)\n",
        "        \n",
        "        ae_model = Model(inputs=_x_in, outputs = _x_out, name='AutoencoderModel')\n",
        "        enc_model = Model(inputs=_x_in, outputs = _z_out, name='EncoderModel')\n",
        "        return ae_model, enc_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZvu54cbjlc8"
      },
      "source": [
        "# 프로젝트 경로 설정 \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/data/\"\n",
        "\n",
        "project_dir = os.path.abspath(project_dir)\n",
        "\n",
        "if project_dir not in sys.path:\n",
        "\n",
        "    sys.path.append(project_dir)\n",
        "\n",
        "data_dir = os.path.join(project_dir, 'real_data/')\n",
        "# 경로를 합친다 ==> /content/drive/MyDrive/data/real_data/ + data/\n",
        "save_dir = os.path.join(project_dir, 'real_save/')\n",
        "\n",
        "# os.makedirs(data_dir, exist_ok=True)\n",
        "# os.makedirs  ==> 파일 Dir 생성\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "# os.makedirs  ==> 파일 Dir 생성"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NCJ3leajpzc"
      },
      "source": [
        "# 데이터\n",
        "\n",
        "data_filenames = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "# os.listdir ==> 디렉토리 내부에 파일 이름을 반환한다.\n",
        "\n",
        "data_filenames\n",
        "\n",
        "df = pd.read_csv(data_dir+'303_1008.csv')\n",
        "\n",
        "df = df.sort_values(by='Time')\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhxAxztCjtWg"
      },
      "source": [
        "# 데이터 타입 및 변수 정의\n",
        "\n",
        "# types = pd.DataFrame(df.dtypes, columns=['type'])\n",
        "\n",
        "# obj_var = list(types[types['type']=='object'].index)\n",
        "\n",
        "# num_var = list(set(df.columns) - set(obj_var))\n",
        "\n",
        "# target = df['Score']\n",
        "\n",
        "# df['Date'] = [_d.split(' ')[0] for _d in df['Date']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qNIdg1gjwGB"
      },
      "source": [
        "# 사용할 컬럼 \n",
        "sensor = ['X','Y','Z']\n",
        "# 사용하지 않을 컬럼\n",
        "# drop_col = ['time']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr5EF1rIjwrk"
      },
      "source": [
        "# def process_window(signal, label, size, slide=None):\n",
        "\n",
        "def process_window(signal, size, slide=None):\n",
        "    # df_ID_date[sensor].values /  df_ID_date['y'].values / 8 / none\n",
        "\n",
        "    # assert len(signal) == len(label)\n",
        "\n",
        "    if slide is None:\n",
        "\n",
        "        slide = 4\n",
        "        # size =  8\n",
        "        \n",
        "\n",
        "    # x, y = [], []\n",
        "    x = []\n",
        "\n",
        "    for start in range(0, len(signal)-size, slide):\n",
        "\n",
        "        end = start + size\n",
        "\n",
        "        x.append(signal[start:end])\n",
        "\n",
        "        # y.append(label[start:end].any() + 0)\n",
        "\n",
        "    # return np.array(x), np.array(y)\n",
        "    return np.array(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVtaso6RjzGg"
      },
      "source": [
        "X= []\n",
        "\n",
        "scale_col = ['X','Y','Z']\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "df[scale_col] = std.fit_transform(df[scale_col].values)\n",
        "\n",
        "df[scale_col] = pd.DataFrame(df[scale_col],index=df[scale_col].index, columns=df[scale_col].columns)\n",
        "# sensor = ['X','Y','Z']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWsun4St3LVq"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JjNK1rvbaNl"
      },
      "source": [
        "# # for ID in df['time'].unique():\n",
        "\n",
        "# #     df_ID = df[df['time'] == time]\n",
        "\n",
        "# for date in df['Time'].unique():\n",
        "\n",
        "#     df_ID_date = df\n",
        "#     _X = process_window(df_ID_date[sensor].values, 100)\n",
        "\n",
        "#     if len(_X) > 0:\n",
        "\n",
        "#         X.append(_X)\n",
        "     \n",
        "#         # y.append(_y)\n",
        "\n",
        "# X = np.row_stack(X)\n",
        "# # y = np.concatenate(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DD7YtEDoY5H"
      },
      "source": [
        "df_ID_date = df\n",
        "_X = process_window(df_ID_date[sensor].values, 8)\n",
        "\n",
        "if len(_X) > 0:\n",
        "    X.append(_X)\n",
        "\n",
        "X = np.row_stack(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azdfEjMerP6u"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iJDS7oW29ky"
      },
      "source": [
        "X.shape\n",
        "x = np.expand_dims(X, 2)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ0mmMOAaP_6"
      },
      "source": [
        "\n",
        "#np.unique(y)\n",
        "\n",
        "\n",
        "# model fit\n",
        "# n_cluster cluster 개수 지정\n",
        "\n",
        "dec = DEC(n_clusters=8)\n",
        "dec.train(x)\n",
        "\n",
        "a = dec.predict(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPOhMX9Lthqu"
      },
      "source": [
        "kmean_df = pd.DataFrame(a)\n",
        "kmean_df\n",
        "\n",
        "x = kmean_df[0].unique()\n",
        "y = kmean_df[0].value_counts()\n",
        "y = y.sort_index()\n",
        "\n",
        "plt.bar(x, y, width=0.5)\n",
        "\n",
        "for i, v in enumerate(x):\n",
        "    plt.text(v, y[i], y[i],                 # 좌표 (x축 = v, y축 = y[0]..y[1], 표시 = y[0]..y[1])\n",
        "             fontsize = 9, \n",
        "             color='black',\n",
        "             horizontalalignment='center',  # horizontalalignment (left, center, right)\n",
        "             verticalalignment='bottom')    # verticalalignment (top, center, bottom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP2nJTS54Ptu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "np.unique(a, return_counts=True)\n",
        "# bb = confusion_matrix(y, a)\n",
        "\n",
        "scores_mean = []\n",
        "scores_max = []\n",
        "for c in np.unique(a):\n",
        "    x_c = x[a == c]\n",
        "    scores_mean.append(dec._get_recon_error(x_c).mean(1).mean())\n",
        "    scores_max.append(dec._get_recon_error(x_c).max(1).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "squr9FDc1H1g"
      },
      "source": [
        "scores_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5N8J4Zdj6dw"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_title('DEC Results')\n",
        "ax1.set_xlabel('Cluster')\n",
        "ax1.set_ylabel('Average Reconstruction Error')\n",
        "ax1.plot(scores_mean, marker='x')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Number of Anomalies')\n",
        "ax2.plot(bb[1], marker='x', color='red')\n",
        "plt.show()\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Number of Anomalies')\n",
        "ax2.plot(bb[1], marker='x', color='red')\n",
        "plt.show()\n",
        "print(bb[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a__PwB93gBZ"
      },
      "source": [
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_title('DEC Results')\n",
        "ax1.set_xlabel('Cluster')\n",
        "ax1.set_ylabel('Max Channel-Reconstruction Error')\n",
        "ax1.plot(scores_max, marker='x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWyXLDfyqxs6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}